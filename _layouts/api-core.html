<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>quickemcee.core API documentation</title>
<meta name="description" content="Main file with the core scripts." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>quickemcee.core</code></h1>
</header>
<section id="section-intro">
<p>Main file with the core scripts.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Main file with the core scripts.&#34;&#34;&#34;

import numpy as np
import emcee
from multiprocessing import Pool


class Model:
    &#34;&#34;&#34;Build a model object.&#34;&#34;&#34;

    def __init__(self, ndim, predict, priors, y_data, y_sigma):
        &#34;&#34;&#34;
        Parameters
        ----------
        ndim : int
            Number of parameters to fit in the model.
        predict : callable
            predict is a callable that takes as argument a 1D array of length
            ndim and returns an array of the same shape as y_data. This object
            should compute the prediction of the model for a vector in the
            parameter space.
        priors : list
            A list whose elements are callables that compute the prior
            probability distribution function of one of the model&#39;s parameters
            when called with a float as argument. The list must be ordered in
            the same way that predict arguments are.
        y_data : array
            array with the target data used to fit the model.
        y_sigma : float or array
            float with a single value for all elements in y_data or array of
            the same shape of y_data with a value for each element in y_data.

        &#34;&#34;&#34;
        self.predict, self.ndim, self.priors = (predict, ndim, priors)
        self.y_data, self.y_sigma = (y_data, y_sigma)

    def _log_prior(self, coords):
        &#34;&#34;&#34;
        Compute log prior.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the
            parameters space for which log prior is computed.

        Returns
        -------
        float

        &#34;&#34;&#34;
        lp = 1
        for i in range(self.ndim):
            iprob = self.priors[i](coords[i])
            if iprob &lt;= 0:
                return -np.inf
            else:
                lp = lp * iprob
        return np.log(lp)

    def _log_likelihood(self, coords):
        &#34;&#34;&#34;
        Compute log likelihood.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the parameters space
            for which log likelihood is computed.

        Returns
        -------
        float

        &#34;&#34;&#34;
        # discard diverging simulations
        prediction = self.predict(coords)
        if not np.isfinite(prediction).all():
            return -np.inf

        return -0.5 * np.sum(((prediction - self.y_data) / self.y_sigma) ** 2)

    def _log_probability(self, coords):
        &#34;&#34;&#34;
        Compute log probability.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the parameters space
            for which log probability is computed.

        Returns
        -------
        float

        &#34;&#34;&#34;
        lp = self._log_prior(coords)

        if not np.isfinite(lp):
            return -np.inf

        return lp + self._log_likelihood(coords)

    def compute_neg_ll(self, coords):
        &#34;&#34;&#34;
        Compute the negative log likelihood function.

        This function is often used to improve the initial guess with an
        external optimizer. In some cases this strategy can greatly reduce
        the necessary length for the burn in phase, thus reducing time spend
        doing calculations.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the parameters space
            for which neg log likelihood is computed.

        Returns
        -------
        float

        Examples
        --------
            from scipy.optimize import minimize
            f = mymodel.compute_neg_ll
            results = minimize(f, x0) # input the rough initial guess as x0
            x0 = results.x # extract an improved initial guess
                           # from the results object
            mymodel.run_chain(nwalkers, shorter_burn_iter, main_iter,
                              init_x=x0)

        Other useful external optimizers are `dual_annealing` and
        `differential_evolution`, also from `scipy.optimize`.
        &#34;&#34;&#34;
        return -self._log_likelihood(coords)

    def run_chain(self, nwalkers, burn_iter, main_iter,
                  init_x=None, moves=None, workers=1):
        &#34;&#34;&#34;
        Instance an `emcee` Ensemble Sambler and run an MCMC chain with it.

        Parameters
        ----------
        nwalkers : int
            number of walkers.
        burn_iter : int
            the number of steps that the chain will do during the burn in
            phase. The samples produced during burn in phase are discarded.
        main_iter : int
            the number of steps that the chain will do during the production
            phase. The samples produced during production phase are saved in
            the sampler and can be extracted for later analysis.
        init_x : array, optional
            1D array of length ndim with an initial guess for the parameters
            values. When set as None uses all zeroes. The default is None.
        moves : emcee moves object, optional
            `emcee` moves object. The default is None.
        workers : int, optional
            Parallelize the computing by setting up a pool of workers of size
            workers. The default is 1.

        Returns
        -------
        sampler : emcee Ensemble Sampler object
            The instanced `emcee` sampler for which the chain is run.

        &#34;&#34;&#34;
        ndim = self.ndim

        if init_x is None:
            init_x = np.zeros(ndim)

        p0 = [init_x + 1e-7 * np.random.randn(ndim)
              for i in range(nwalkers)]

        if workers == 1:
            sampler = emcee.EnsembleSampler(nwalkers,
                                            ndim,
                                            self._log_probability,
                                            moves=moves)
            print(&#34;&#34;)
            print(&#34;Running burn-in...&#34;)
            p0, _, _ = sampler.run_mcmc(p0, burn_iter, progress=True)
            sampler.reset()

            print(&#34;&#34;)
            print(&#34;Running production...&#34;)
            pos, prob, state = sampler.run_mcmc(p0, main_iter, progress=True)

            return sampler

        elif workers &gt; 1:
            with Pool(processes=workers) as pool:
                sampler = emcee.EnsembleSampler(nwalkers,
                                                ndim,
                                                self._log_probability,
                                                moves=moves,
                                                pool=pool)
                print(&#34;&#34;)
                print(&#34;Running burn-in...&#34;)
                p0, _, _ = sampler.run_mcmc(p0, burn_iter, progress=True)
                sampler.reset()

                print(&#34;&#34;)
                print(&#34;Running production...&#34;)
                pos, prob, state = sampler.run_mcmc(p0, main_iter,
                                                    progress=True)
                pool.close()
            # outside with-as
            return sampler</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="quickemcee.core.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>ndim, predict, priors, y_data, y_sigma)</span>
</code></dt>
<dd>
<div class="desc"><p>Build a model object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ndim</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of parameters to fit in the model.</dd>
<dt><strong><code>predict</code></strong> :&ensp;<code>callable</code></dt>
<dd>predict is a callable that takes as argument a 1D array of length
ndim and returns an array of the same shape as y_data. This object
should compute the prediction of the model for a vector in the
parameter space.</dd>
<dt><strong><code>priors</code></strong> :&ensp;<code>list</code></dt>
<dd>A list whose elements are callables that compute the prior
probability distribution function of one of the model's parameters
when called with a float as argument. The list must be ordered in
the same way that predict arguments are.</dd>
<dt><strong><code>y_data</code></strong> :&ensp;<code>array</code></dt>
<dd>array with the target data used to fit the model.</dd>
<dt><strong><code>y_sigma</code></strong> :&ensp;<code>float</code> or <code>array</code></dt>
<dd>float with a single value for all elements in y_data or array of
the same shape of y_data with a value for each element in y_data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model:
    &#34;&#34;&#34;Build a model object.&#34;&#34;&#34;

    def __init__(self, ndim, predict, priors, y_data, y_sigma):
        &#34;&#34;&#34;
        Parameters
        ----------
        ndim : int
            Number of parameters to fit in the model.
        predict : callable
            predict is a callable that takes as argument a 1D array of length
            ndim and returns an array of the same shape as y_data. This object
            should compute the prediction of the model for a vector in the
            parameter space.
        priors : list
            A list whose elements are callables that compute the prior
            probability distribution function of one of the model&#39;s parameters
            when called with a float as argument. The list must be ordered in
            the same way that predict arguments are.
        y_data : array
            array with the target data used to fit the model.
        y_sigma : float or array
            float with a single value for all elements in y_data or array of
            the same shape of y_data with a value for each element in y_data.

        &#34;&#34;&#34;
        self.predict, self.ndim, self.priors = (predict, ndim, priors)
        self.y_data, self.y_sigma = (y_data, y_sigma)

    def _log_prior(self, coords):
        &#34;&#34;&#34;
        Compute log prior.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the
            parameters space for which log prior is computed.

        Returns
        -------
        float

        &#34;&#34;&#34;
        lp = 1
        for i in range(self.ndim):
            iprob = self.priors[i](coords[i])
            if iprob &lt;= 0:
                return -np.inf
            else:
                lp = lp * iprob
        return np.log(lp)

    def _log_likelihood(self, coords):
        &#34;&#34;&#34;
        Compute log likelihood.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the parameters space
            for which log likelihood is computed.

        Returns
        -------
        float

        &#34;&#34;&#34;
        # discard diverging simulations
        prediction = self.predict(coords)
        if not np.isfinite(prediction).all():
            return -np.inf

        return -0.5 * np.sum(((prediction - self.y_data) / self.y_sigma) ** 2)

    def _log_probability(self, coords):
        &#34;&#34;&#34;
        Compute log probability.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the parameters space
            for which log probability is computed.

        Returns
        -------
        float

        &#34;&#34;&#34;
        lp = self._log_prior(coords)

        if not np.isfinite(lp):
            return -np.inf

        return lp + self._log_likelihood(coords)

    def compute_neg_ll(self, coords):
        &#34;&#34;&#34;
        Compute the negative log likelihood function.

        This function is often used to improve the initial guess with an
        external optimizer. In some cases this strategy can greatly reduce
        the necessary length for the burn in phase, thus reducing time spend
        doing calculations.

        Parameters
        ----------
        coords : array
            A 1D array of length ndim with the vector in the parameters space
            for which neg log likelihood is computed.

        Returns
        -------
        float

        Examples
        --------
            from scipy.optimize import minimize
            f = mymodel.compute_neg_ll
            results = minimize(f, x0) # input the rough initial guess as x0
            x0 = results.x # extract an improved initial guess
                           # from the results object
            mymodel.run_chain(nwalkers, shorter_burn_iter, main_iter,
                              init_x=x0)

        Other useful external optimizers are `dual_annealing` and
        `differential_evolution`, also from `scipy.optimize`.
        &#34;&#34;&#34;
        return -self._log_likelihood(coords)

    def run_chain(self, nwalkers, burn_iter, main_iter,
                  init_x=None, moves=None, workers=1):
        &#34;&#34;&#34;
        Instance an `emcee` Ensemble Sambler and run an MCMC chain with it.

        Parameters
        ----------
        nwalkers : int
            number of walkers.
        burn_iter : int
            the number of steps that the chain will do during the burn in
            phase. The samples produced during burn in phase are discarded.
        main_iter : int
            the number of steps that the chain will do during the production
            phase. The samples produced during production phase are saved in
            the sampler and can be extracted for later analysis.
        init_x : array, optional
            1D array of length ndim with an initial guess for the parameters
            values. When set as None uses all zeroes. The default is None.
        moves : emcee moves object, optional
            `emcee` moves object. The default is None.
        workers : int, optional
            Parallelize the computing by setting up a pool of workers of size
            workers. The default is 1.

        Returns
        -------
        sampler : emcee Ensemble Sampler object
            The instanced `emcee` sampler for which the chain is run.

        &#34;&#34;&#34;
        ndim = self.ndim

        if init_x is None:
            init_x = np.zeros(ndim)

        p0 = [init_x + 1e-7 * np.random.randn(ndim)
              for i in range(nwalkers)]

        if workers == 1:
            sampler = emcee.EnsembleSampler(nwalkers,
                                            ndim,
                                            self._log_probability,
                                            moves=moves)
            print(&#34;&#34;)
            print(&#34;Running burn-in...&#34;)
            p0, _, _ = sampler.run_mcmc(p0, burn_iter, progress=True)
            sampler.reset()

            print(&#34;&#34;)
            print(&#34;Running production...&#34;)
            pos, prob, state = sampler.run_mcmc(p0, main_iter, progress=True)

            return sampler

        elif workers &gt; 1:
            with Pool(processes=workers) as pool:
                sampler = emcee.EnsembleSampler(nwalkers,
                                                ndim,
                                                self._log_probability,
                                                moves=moves,
                                                pool=pool)
                print(&#34;&#34;)
                print(&#34;Running burn-in...&#34;)
                p0, _, _ = sampler.run_mcmc(p0, burn_iter, progress=True)
                sampler.reset()

                print(&#34;&#34;)
                print(&#34;Running production...&#34;)
                pos, prob, state = sampler.run_mcmc(p0, main_iter,
                                                    progress=True)
                pool.close()
            # outside with-as
            return sampler</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="quickemcee.core.Model.compute_neg_ll"><code class="name flex">
<span>def <span class="ident">compute_neg_ll</span></span>(<span>self, coords)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the negative log likelihood function.</p>
<p>This function is often used to improve the initial guess with an
external optimizer. In some cases this strategy can greatly reduce
the necessary length for the burn in phase, thus reducing time spend
doing calculations.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>coords</code></strong> :&ensp;<code>array</code></dt>
<dd>A 1D array of length ndim with the vector in the parameters space
for which neg log likelihood is computed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>from scipy.optimize import minimize
f = mymodel.compute_neg_ll
results = minimize(f, x0) # input the rough initial guess as x0
x0 = results.x # extract an improved initial guess
               # from the results object
mymodel.run_chain(nwalkers, shorter_burn_iter, main_iter,
                  init_x=x0)
</code></pre>
<p>Other useful external optimizers are <code>dual_annealing</code> and
<code>differential_evolution</code>, also from <code>scipy.optimize</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_neg_ll(self, coords):
    &#34;&#34;&#34;
    Compute the negative log likelihood function.

    This function is often used to improve the initial guess with an
    external optimizer. In some cases this strategy can greatly reduce
    the necessary length for the burn in phase, thus reducing time spend
    doing calculations.

    Parameters
    ----------
    coords : array
        A 1D array of length ndim with the vector in the parameters space
        for which neg log likelihood is computed.

    Returns
    -------
    float

    Examples
    --------
        from scipy.optimize import minimize
        f = mymodel.compute_neg_ll
        results = minimize(f, x0) # input the rough initial guess as x0
        x0 = results.x # extract an improved initial guess
                       # from the results object
        mymodel.run_chain(nwalkers, shorter_burn_iter, main_iter,
                          init_x=x0)

    Other useful external optimizers are `dual_annealing` and
    `differential_evolution`, also from `scipy.optimize`.
    &#34;&#34;&#34;
    return -self._log_likelihood(coords)</code></pre>
</details>
</dd>
<dt id="quickemcee.core.Model.run_chain"><code class="name flex">
<span>def <span class="ident">run_chain</span></span>(<span>self, nwalkers, burn_iter, main_iter, init_x=None, moves=None, workers=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Instance an <code>emcee</code> Ensemble Sambler and run an MCMC chain with it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>nwalkers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of walkers.</dd>
<dt><strong><code>burn_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of steps that the chain will do during the burn in
phase. The samples produced during burn in phase are discarded.</dd>
<dt><strong><code>main_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of steps that the chain will do during the production
phase. The samples produced during production phase are saved in
the sampler and can be extracted for later analysis.</dd>
<dt><strong><code>init_x</code></strong> :&ensp;<code>array</code>, optional</dt>
<dd>1D array of length ndim with an initial guess for the parameters
values. When set as None uses all zeroes. The default is None.</dd>
<dt><strong><code>moves</code></strong> :&ensp;<code>emcee moves object</code>, optional</dt>
<dd><code>emcee</code> moves object. The default is None.</dd>
<dt><strong><code>workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parallelize the computing by setting up a pool of workers of size
workers. The default is 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sampler</code></strong> :&ensp;<code>emcee Ensemble Sampler object</code></dt>
<dd>The instanced <code>emcee</code> sampler for which the chain is run.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_chain(self, nwalkers, burn_iter, main_iter,
              init_x=None, moves=None, workers=1):
    &#34;&#34;&#34;
    Instance an `emcee` Ensemble Sambler and run an MCMC chain with it.

    Parameters
    ----------
    nwalkers : int
        number of walkers.
    burn_iter : int
        the number of steps that the chain will do during the burn in
        phase. The samples produced during burn in phase are discarded.
    main_iter : int
        the number of steps that the chain will do during the production
        phase. The samples produced during production phase are saved in
        the sampler and can be extracted for later analysis.
    init_x : array, optional
        1D array of length ndim with an initial guess for the parameters
        values. When set as None uses all zeroes. The default is None.
    moves : emcee moves object, optional
        `emcee` moves object. The default is None.
    workers : int, optional
        Parallelize the computing by setting up a pool of workers of size
        workers. The default is 1.

    Returns
    -------
    sampler : emcee Ensemble Sampler object
        The instanced `emcee` sampler for which the chain is run.

    &#34;&#34;&#34;
    ndim = self.ndim

    if init_x is None:
        init_x = np.zeros(ndim)

    p0 = [init_x + 1e-7 * np.random.randn(ndim)
          for i in range(nwalkers)]

    if workers == 1:
        sampler = emcee.EnsembleSampler(nwalkers,
                                        ndim,
                                        self._log_probability,
                                        moves=moves)
        print(&#34;&#34;)
        print(&#34;Running burn-in...&#34;)
        p0, _, _ = sampler.run_mcmc(p0, burn_iter, progress=True)
        sampler.reset()

        print(&#34;&#34;)
        print(&#34;Running production...&#34;)
        pos, prob, state = sampler.run_mcmc(p0, main_iter, progress=True)

        return sampler

    elif workers &gt; 1:
        with Pool(processes=workers) as pool:
            sampler = emcee.EnsembleSampler(nwalkers,
                                            ndim,
                                            self._log_probability,
                                            moves=moves,
                                            pool=pool)
            print(&#34;&#34;)
            print(&#34;Running burn-in...&#34;)
            p0, _, _ = sampler.run_mcmc(p0, burn_iter, progress=True)
            sampler.reset()

            print(&#34;&#34;)
            print(&#34;Running production...&#34;)
            pos, prob, state = sampler.run_mcmc(p0, main_iter,
                                                progress=True)
            pool.close()
        # outside with-as
        return sampler</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="quickemcee" href="index.html">quickemcee</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="quickemcee.core.Model" href="#quickemcee.core.Model">Model</a></code></h4>
<ul class="">
<li><code><a title="quickemcee.core.Model.compute_neg_ll" href="#quickemcee.core.Model.compute_neg_ll">compute_neg_ll</a></code></li>
<li><code><a title="quickemcee.core.Model.run_chain" href="#quickemcee.core.Model.run_chain">run_chain</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
